{"cells":[{"cell_type":"markdown","source":["Load Data"],"metadata":{"id":"UwIkLaF7F6jj"}},{"cell_type":"code","source":["import pandas as pd\n","personal_data = pd.read_csv('https://storage.googleapis.com/datalynn-datasets/Interview_Challenge/JPMorgan/JPMorgan_credit_resks_prediction/personal_data.csv')\n","credit_data = pd.read_csv('https://storage.googleapis.com/datalynn-datasets/Interview_Challenge/JPMorgan/JPMorgan_credit_resks_prediction/credit_data.csv')\n","default_data = pd.read_csv('https://storage.googleapis.com/datalynn-datasets/Interview_Challenge/JPMorgan/JPMorgan_credit_resks_prediction/default_data.csv')\n"],"metadata":{"id":"9Jig_eDOF6Tc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TnpeMsJ43tJt"},"source":["## 3. Questions for the Data Challenge##\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2-Juu4tS37Td"},"source":["###1. Data Manipulation (SQL)###\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QkyMnwWx7YXJ"},"source":["**Question**: Suppose we want to evaluate the overall creditworthiness of our clients. However, for some customers, the Credit_Score information in the Credit_Data table is missing. Can you generate a query that provides a list of all CustomerIDs where the Credit_Score data is not yet available?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEWM59dk7b1A"},"outputs":[],"source":["import sqlite3\n","\n","# Create an in-memory SQLite database\n","conn = sqlite3.connect(':memory:')\n","\n","# Store the DataFrames in the database as tables\n","personal_data.to_sql('personal_data', conn, index=False)\n","credit_data.to_sql('credit_data', conn, index=False)\n","default_records.to_sql('default_records', conn, index=False)\n","\n","# Checking for Null values\n","query = '''\n","SELECT\n","    CustomerID\n","FROM\n","    Credit_Data\n","WHERE\n","    Credit_Score IS NULL;\n","\n","'''\n","\n","conn.execute(query)\n","conn.commit()\n","\n","# Retrieve the updated dataframe from the database\n","credit_data = pd.read_sql_query(\"SELECT * FROM Credit_Data\", conn)\n","\n","# Close the database connection\n","conn.close()"]},{"cell_type":"markdown","metadata":{"id":"f0AhY-7y-QsT"},"source":["**Question**: Using the Credit_Data table, can you calculate the cumulative loan amount for each customer, ordered by the loan term? What do you observe about the patterns of borrowing over time for customers?"]},{"cell_type":"markdown","metadata":{"id":"kAwCkNIeRFfc"},"source":["**Answer**:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqsJ4dhIRKQr"},"outputs":[],"source":["import sqlite3\n","\n","# Create an in-memory SQLite database\n","conn = sqlite3.connect(':memory:')\n","\n","# Store the DataFrames in the database as tables\n","personal_data.to_sql('personal_data', conn, index=False)\n","credit_data.to_sql('credit_data', conn, index=False)\n","default_records.to_sql('default_records', conn, index=False)\n","\n","# Checking for Null values\n","query = '''\n","SELECT COUNT(*)\n","FROM Credit_Data\n","WHERE CustomerID IS NULL OR Loan_Term IS NULL OR Loan_Amount IS NULL;\n","'''\n","\n","# Checking for duplicates\n","query = '''\n","SELECT CustomerID, Loan_Term, COUNT(*)\n","FROM Credit_Data\n","GROUP BY CustomerID, Loan_Term\n","HAVING COUNT(*) > 1;\n","'''\n","\n","# Calculate the cumulative loan amount for each customer ordered by the loan term\n","query = '''\n","SELECT\n","    CustomerID,\n","    Loan_Term,\n","    Loan_Amount,\n","    SUM(Loan_Amount) OVER (\n","        PARTITION BY CustomerID\n","        ORDER BY Loan_Term\n","        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n","    ) AS Cumulative_Loan_Amount\n","FROM Credit_Data\n","ORDER BY CustomerID, Loan_Term;\n","'''\n","\n","conn.execute(query)\n","conn.commit()\n","\n","# Retrieve the updated dataframe from the database\n","credit_data = pd.read_sql_query(\"SELECT * FROM Credit_Data\", conn)\n","\n","# Close the database connection\n","conn.close()"]},{"cell_type":"markdown","metadata":{"id":"oJrjGR164KUZ"},"source":["###2. Feature Processing and Feature Engineering (Python)###\n","\n"]},{"cell_type":"markdown","metadata":{"id":"peCKs_udWF1h"},"source":["**Question**: In the context of predicting credit default risks for potential loan borrowers, we have numerical features like 'Income', 'Loan_Amount', 'Credit_Score', and 'Age', and categorical features like 'Occupation', 'Education', 'Loan_Type'. How would you prepare these features for model training, specifically handling the categorical variables?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rssaR2xUgcPb","outputId":"6b38414a-f0a2-4c4d-f316-f54445116f39"},"outputs":[{"name":"stdout","output_type":"stream","text":["Occupation:\n","Lawyer      261\n","Engineer    254\n","Teacher     252\n","Doctor      233\n","Name: Occupation, dtype: int64\n","--------------------------------------------------\n","Education:\n","PhD              279\n","Graduate         249\n","High School      244\n","Undergraduate    228\n","Name: Education, dtype: int64\n","--------------------------------------------------\n","Loan_Type:\n","Home        346\n","Personal    330\n","Auto        324\n","Name: Loan_Type, dtype: int64\n","--------------------------------------------------\n","Loan_Term:\n","Medium    374\n","Short     345\n","Long      281\n","Name: Loan_Term, dtype: int64\n","--------------------------------------------------\n"]}],"source":["# Merge the three dataframes based on 'CustomerID'\n","df = personal_data.merge(credit_data, on='CustomerID', how='inner')\n","df = df.merge(default_records, on='CustomerID', how='inner')\n","\n","# Data Understanding\n","for col in ['Occupation', 'Education', 'Loan_Type','Loan_Term']:\n","    print(f\"{col}:\\n{df[col].value_counts()}\\n{'-'*50}\")\n","\n","# Handle Missing Values\n","df['Occupation'] = df['Occupation'].fillna('Unknown')\n","\n","# Encoding\n","df = pd.get_dummies(df, columns=['Occupation', 'Education', 'Loan_Type','Loan_Term'])"]},{"cell_type":"markdown","metadata":{"id":"6nUOxxKW4NZG"},"source":["###3. Modeling Metrics (Python)###\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"llP4xRaoQJIo"},"source":["**Question**: In credit default prediction, different types of errors have different costs. For instance, predicting a customer will not default when they actually will (False Negative) might be more costly than predicting a customer will default when they actually will not (False Positive). Given these considerations, what kind of performance metric would you suggest that we use instead of traditional ones like accuracy? Explain your choice and how it might be beneficial in this particular business context. Additionally, write a Python function to calculate this metric given the true labels and the model predictions."]},{"cell_type":"markdown","metadata":{"id":"g4F_rYNv4POw"},"source":["###4. Machine Learning Modeling and Model Optimization(Python)###\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aGBTXjFcNBq8"},"source":["**Question**: For our credit default prediction task, assume you have decided to initially start with Logistic Regression. With this model selection, what pre-training manipulation would implement? Why would you implement feature scaling for this model? Why is scaling important? Except Logistic Regression, what other models would require scaling? Please provide Python code to demonstrate."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tAGPbAIrm0fH","outputId":"6c7357a4-495d-4b2f-c001-d8b59e972805"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training F1 Score: 0.0\n","Testing F1 Score: 0.0\n"]}],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","\n","# Assume df is the DataFrame containing the data\n","\n","# Let's split the data into features and target variable\n","X = df.drop('Default', axis=1)\n","y = df['Default']\n","\n","# Split the data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the StandardScaler\n","scaler = StandardScaler()\n","\n","# Fit and transform the training data\n","X_train_scaled = scaler.fit_transform(X_train)\n","\n","# Transform the test data\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Initialize and fit the Logistic Regression model\n","log_reg = LogisticRegression()\n","log_reg.fit(X_train_scaled, y_train)\n","\n","# Generate predictions on the training and testing data\n","y_train_pred = log_reg.predict(X_train_scaled)\n","y_test_pred = log_reg.predict(X_test_scaled)\n","\n","# Calculate the F1 scores\n","f1_train = f1_score(y_train, y_train_pred)\n","f1_test = f1_score(y_test, y_test_pred)\n","\n","print(\"Training F1 Score:\", f1_train)\n","print(\"Testing F1 Score:\", f1_test)"]},{"cell_type":"markdown","metadata":{"id":"iikRh4umOudf"},"source":["**Question**: Assume you have decided to optimize your model by using XGBoost instead of Logistic Regression. XGBoost is known for its speed and performance but it has quite a few hyperparameters to tune. How do you approach this? And what's your strategy to ensure you're not overfitting the model?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":248},"id":"HHajS0Te3Awl","outputId":"400b76e5-072a-491f-bbcd-4643bad0c19d"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n","              colsample_bylevel=None, colsample_bynode=None,\n","              colsample_bytree=0.7, early_stopping_rounds=None,\n","              enable_categorical=False, eval_metric=None, feature_types=None,\n","              gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n","              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n","              max_cat_threshold=None, max_cat_to_onehot=None,\n","              max_delta_step=None, max_depth=5, max_leaves=None,\n","              min_child_weight=None, missing=nan, monotone_constraints=None,\n","              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n","              predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n","              colsample_bylevel=None, colsample_bynode=None,\n","              colsample_bytree=0.7, early_stopping_rounds=None,\n","              enable_categorical=False, eval_metric=None, feature_types=None,\n","              gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n","              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n","              max_cat_threshold=None, max_cat_to_onehot=None,\n","              max_delta_step=None, max_depth=5, max_leaves=None,\n","              min_child_weight=None, missing=nan, monotone_constraints=None,\n","              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n","              predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"],"text/plain":["XGBClassifier(base_score=None, booster=None, callbacks=None,\n","              colsample_bylevel=None, colsample_bynode=None,\n","              colsample_bytree=0.7, early_stopping_rounds=None,\n","              enable_categorical=False, eval_metric=None, feature_types=None,\n","              gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n","              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n","              max_cat_threshold=None, max_cat_to_onehot=None,\n","              max_delta_step=None, max_depth=5, max_leaves=None,\n","              min_child_weight=None, missing=nan, monotone_constraints=None,\n","              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n","              predictor=None, random_state=None, ...)"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.model_selection import GridSearchCV\n","from xgboost import XGBClassifier\n","\n","# Initialize XGBoost classifier\n","model = XGBClassifier()\n","\n","# Define the hyperparameters\n","param_grid = {\n","    'n_estimators': [100, 200, 500],\n","    'learning_rate': [0.01, 0.05, 0.1],\n","    'max_depth': [3, 5, 7],\n","    'colsample_bytree': [0.3, 0.5, 0.7],\n","    'gamma': [0, 0.1, 0.2]\n","}\n","\n","# Set up the grid search\n","grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc')\n","\n","# Conduct the grid search\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best parameters\n","best_params = grid_search.best_params_\n","\n","# Fit the model with the best parameters to the full training set\n","model = XGBClassifier(**best_params)\n","model.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hOT0ME1B3FpU","outputId":"505a850e-d7f9-412c-97c1-4dda0eb2a79f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training F1 Score: 0.017699115044247787\n","Testing F1 Score: 0.044444444444444446\n"]}],"source":["# Generate predictions on the training and testing data\n","y_train_pred = model.predict(X_train_scaled)\n","y_test_pred = model.predict(X_test_scaled)\n","\n","# Calculate the F1 scores\n","f1_train = f1_score(y_train, y_train_pred)\n","f1_test = f1_score(y_test, y_test_pred)\n","\n","print(\"Training F1 Score:\", f1_train)\n","print(\"Testing F1 Score:\", f1_test)"]},{"cell_type":"markdown","metadata":{"id":"qk9phBRLXeXc"},"source":["**Question**: Assume with XGBoost, our model's AUC-ROC score is high, but when we deploy it in production, the actual positive predictive value (precision) is much lower than expected. What could explain this discrepancy and how would you investigate?"]},{"cell_type":"markdown","metadata":{"id":"FGkwnzB14RCF"},"source":["###5. Business Insights (Python)###\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iY4hZtoRQPuS"},"source":["**Question**:Given the model results, which features appear to be the most significant predictors of loan default? How would you communicate these insights to business stakeholders in order to shape future lending strategies?\n","\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}